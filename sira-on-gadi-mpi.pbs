#!/bin/bash
#PBS -N ramsey-EPN-mpi-run
#PBS -m be
#PBS -P w84
#PBS -q normalbw
#PBS -l walltime=12:00:00
#PBS -l ncpus=240
#PBS -l mem=1280GB
#PBS -l jobfs=50GB
# Request iointensive NVMe-over-fabrics volumes (1 TiB each), divisible by node count.
# For normalbw (28 cores/node), ncpus=240 typically implies 9 nodes â†’ request 9 volumes (1 TiB/node).
# Adjust if you change ncpus or queue/node type.
#PBS -l iointensive=9
#PBS -l storage=scratch/y57+scratch/n74+scratch/w84+gdata/w84
#PBS -j oe
#PBS -l wd

set -euo pipefail

# --- User configuration
ASSET="EPN_Yilgarn_microcomps_L3"
CODE_DIR="/scratch/y57/mr3457/code/sira"
MODELS_DIR="/scratch/y57/mr3457/code/_SYSTEM_MODELS/epn"
VENV="$HOME/venv/sira-v25"

# --- Environment setup
module load python3/3.11.7
module load openmpi/4.1.4
source "$VENV/bin/activate"

export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
export NUMEXPR_MAX_THREADS=1

# Additional optimisations for large-scale HPC run
export MALLOC_MMAP_THRESHOLD_=131072
export MALLOC_TRIM_THRESHOLD_=131072
export OMP_WAIT_POLICY=PASSIVE

export SIRA_LOG_LEVEL=INFO
export SIRA_QUIET_MODE=0

# Stream to iointensive (per-node, ultra-low-latency). We'll stage-out later.
export SIRA_STREAM_DIR="/iointensive/sira_${PBS_JOBID}"
mkdir -p "$SIRA_STREAM_DIR"
# Defer in-process consolidation so we can stage-out from per-node storage first
export SIRA_DEFER_CONSOLIDATION=1

export SIRA_SAVE_COMPTYPE_RESPONSE=1
export SIRA_SAVE_COMPONENT_RESPONSE=0

# --- Performance tuning
# Optimised for large hazard file:
export SIRA_CHUNKS_PER_SLOT=1
# Fastest compression for large datasets:
export SIRA_STREAM_COMPRESSION=snappy
# Optimised row groups for large hazards (balance memory vs I/O):
export SIRA_STREAM_ROW_GROUP=524288
# Reproducible results:
export PYTHONHASHSEED=0
# Large hazard optimisation - avoid multiprocessing overhead:
export SIRA_MIN_HAZARDS_FOR_PARALLEL=100000

# --- HPC Optimisations

# Enable HPC optimisations for large-scale run
export SIRA_HPC_MODE=1

# Optimised batch size for <1000 components
export SIRA_MAX_BATCH_SIZE=1000

# Disable progress reporting for cleaner HPC logs
export SIRA_QUIET_MODE=1

# --- Run simulation
cd "$CODE_DIR"
LOGS_DIR="$CODE_DIR/logs"
mkdir -p "$LOGS_DIR"

printf '\n=== SIRA on Gadi ===\n'
printf 'Asset: %s\n' "$ASSET"
printf 'Working directory: %s\n' "$CODE_DIR"
printf 'Logs directory: %s\n' "$LOGS_DIR"
printf 'Stream directory: %s\n' "$SIRA_STREAM_DIR"

python - <<'PYCODE'
from sira.__main__ import safe_mpi_import
MPI, _, rank, size = safe_mpi_import()
if MPI is None:
    print('[x] MPI not available - job will exit')
    raise SystemExit(1)
print(f'[OK] MPI detected: rank {rank} of {size}')
PYCODE

# --- Main SIRA run - MPI backend with optimisations
echo "Starting SIRA (MPI) for large-scale simulation..."
echo "MPI ranks: $PBS_NCPUS"
echo "Start time: $(date)"

# Record start time for performance monitoring
START_TIME=$(date +%s)

# Check initial storage quota
echo "=== Initial Storage Status ==="
nci_account -P "$PROJECT" 2>/dev/null || echo "nci_account not available"
echo ""

# Bind MPI processes to cores for better performance
# Note: Detailed logs are written to <output_dir>/log.txt by the Python logger
mpirun -np "$PBS_NCPUS" --bind-to core --map-by core \
    python -m sira \
    -d "${MODELS_DIR}/${ASSET}/" -s \
    --parallel-backend mpi --stream-results

# Calculate and report execution time
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))
echo "End time: $(date)"
echo "Total execution time (compute): $((DURATION / 3600))h $((DURATION % 3600 / 60))m $((DURATION % 60))s"

# ------------------------------------------------------------------------------
# Stage-out per-node streaming artifacts from iointensive to shared /scratch
# ------------------------------------------------------------------------------

SHARED_BASE="/scratch/${PROJECT}/${USER}/sira_stream_${PBS_JOBID}"
SHARED_STAGE="${SHARED_BASE}/staged"
SHARED_CHUNKS="${SHARED_BASE}/chunks"
mkdir -p "$SHARED_STAGE" "$SHARED_CHUNKS"

echo "Staging-out per-node streaming artifacts to: $SHARED_STAGE"

# Run a tar+copy on each allocated node concurrently
pbsdsh -v bash -lc '
        set -euo pipefail
        SRC="${SIRA_STREAM_DIR:-/iointensive/sira_${PBS_JOBID}}"
        if [ -d "$SRC" ] && [ "$(ls -A "$SRC" 2>/dev/null)" ]; then
                HNAME=$(hostname -s)
                TAR_PATH="/iointensive/sira_${PBS_JOBID}_${HNAME}.tar"
                echo "Packing $SRC on $HNAME -> $TAR_PATH"
                tar -C "$SRC" -cf "$TAR_PATH" .
                echo "Copying $TAR_PATH -> $SHARED_STAGE/"
                cp -f "$TAR_PATH" "$SHARED_STAGE/"
        else
                echo "No streaming files on $(hostname -s); skipping."
        fi
'

echo "Verifying staged artifacts..."
# Expected set of hosts from PBS_NODEFILE (short hostnames)
EXPECTED_HOSTS=$(awk -F. '{print $1}' "$PBS_NODEFILE" | sort -u)
EXPECTED_COUNT=$(echo "$EXPECTED_HOSTS" | wc -l | tr -d ' ')

# Actual set of hosts from staged tar files
ACTUAL_HOSTS=$(ls -1 "$SHARED_STAGE"/sira_${PBS_JOBID}_*.tar 2>/dev/null | \
    sed -E "s|^.*/sira_${PBS_JOBID}_([^/]+)\.tar$|\1|" | sort -u)
ACTUAL_COUNT=$(echo "$ACTUAL_HOSTS" | wc -l | tr -d ' ')

echo "Expected node count: $EXPECTED_COUNT"
echo "Actual staged tars : $ACTUAL_COUNT"

if [ "$EXPECTED_COUNT" -eq 0 ]; then
    echo "[x] Could not determine expected nodes from PBS_NODEFILE. Aborting."
    exit 1
fi

if [ "$ACTUAL_COUNT" -ne "$EXPECTED_COUNT" ]; then
    echo "[x] Verification failed: missing staged tarballs from some nodes."
    echo "Expected hosts:"
    echo "$EXPECTED_HOSTS"
    echo "Actual hosts:"
    echo "$ACTUAL_HOSTS"

    MISSING=$(comm -23 <(echo "$EXPECTED_HOSTS") <(echo "$ACTUAL_HOSTS"))
    if [ -n "$MISSING" ]; then
        echo "Missing hosts:"
        echo "$MISSING"
    fi
    exit 2
fi

echo "Unpacking staged tarballs to: $SHARED_CHUNKS"
for f in "$SHARED_STAGE"/sira_${PBS_JOBID}_*.tar; do
        [ -e "$f" ] || continue
        base=$(basename "$f")
        host=${base#sira_${PBS_JOBID}_}
        host=${host%.tar}
        mkdir -p "$SHARED_CHUNKS/$host"
        tar -xf "$f" -C "$SHARED_CHUNKS/$host"
done

# ------------------------------------------------------------------------------
# Consolidation-only step (now that all chunks are on shared filesystem)
# ------------------------------------------------------------------------------

echo "Starting streaming consolidation from $SHARED_CHUNKS ..."
python - <<'PYCONSOL'
import os, json
from pathlib import Path
from sira.configuration import Configuration
from sira.model_ingest import ingest_model
from sira.modelling.hazard import HazardsContainer
from sira.infrastructure_response import consolidate_streamed_results

asset = os.environ.get('ASSET')
models_dir = os.environ.get('MODELS_DIR')
input_dir = Path(models_dir) / asset / 'input'
# Discover config/model files by pattern to match CLI behaviour
config_file = None
model_file = None
for fname in os.listdir(input_dir):
        if config_file is None and fname.lower().startswith('config') and fname.lower().endswith('.json'):
                config_file = input_dir / fname
        if model_file is None and fname.lower().startswith('model') and fname.lower().endswith('.json'):
                model_file = input_dir / fname
if config_file is None or model_file is None:
        raise SystemExit('Could not find config/model JSON in ' + str(input_dir))

config = Configuration(str(config_file), str(model_file))
infrastructure = ingest_model(config)
hazards_container = HazardsContainer(config, model_file)

stream_dir = os.environ['SHARED_CHUNKS']
scenario = type('ScenarioShim', (), {})()
setattr(scenario, 'num_samples', config.NUM_SAMPLES)
setattr(scenario, 'parallel_config', None)

consolidate_streamed_results(
                stream_dir,
                infrastructure,
                scenario,
                config,
                hazards_container,
                CALC_SYSTEM_RECOVERY=False,
)
PYCONSOL

CONS_END_TIME=$(date +%s)
CONS_DURATION=$((CONS_END_TIME - END_TIME))
echo "Consolidation duration: $((CONS_DURATION / 3600))h $((CONS_DURATION % 3600 / 60))m $((CONS_DURATION % 60))s"

# ------------------------------------------------------------------------------
# Cleanup tail
# ------------------------------------------------------------------------------

echo "Cleaning up staged tarballs..."
rm -f "$SHARED_STAGE"/sira_${PBS_JOBID}_*.tar || true

if [ "${SIRA_CLEANUP_CHUNKS:-0}" = "1" ]; then
    echo "SIRA_CLEANUP_CHUNKS=1 set. Removing shared chunks directory: $SHARED_CHUNKS"
    rm -rf "$SHARED_CHUNKS" || true
else
    echo "Keeping shared chunks at: $SHARED_CHUNKS (set SIRA_CLEANUP_CHUNKS=1 to remove)"
fi

# --- Fallback test with multiprocessing (optional)
# echo "Using multiprocessing backend..."
# python -m sira \
#     -d "${MODELS_DIR}/${ASSET}/" -s \
#     --parallel-backend multiprocessing --stream-results \
#     2>&1 | tee "$LOGS_DIR/sira_fallback.log"

# --- Simple post-run summary
CONFIG_FILE=$(find "${MODELS_DIR}/${ASSET}" -name "config*.json" | head -1)
if [ -n "$CONFIG_FILE" ] && [ -f "$CONFIG_FILE" ]; then
    OUTPUT_DIR_PATH=$(python -c '
import json, sys
try:
    with open(sys.argv[1], "r") as f:
        data = json.load(f)
    path = data.get("OUTPUT_DIR", "./output")
    if path.startswith("./"):
        path = path[2:]
    print(path)
except Exception:
    print("output") # Fallback
' "$CONFIG_FILE")
    OUTPUT_DIR="${MODELS_DIR}/${ASSET}/${OUTPUT_DIR_PATH}"
else
    OUTPUT_DIR="${MODELS_DIR}/${ASSET}/output"
fi

printf '\n=== Outputs ===\n'
printf 'Output directory: %s\n' "$OUTPUT_DIR"
if [ -d "$OUTPUT_DIR" ]; then
    ls -1 "$OUTPUT_DIR"
else
    printf '[x] Output directory not found\n'
fi

# Check final storage quota
printf '\n=== Final Storage Status ===\n'
nci_account -P "$PROJECT" 2>/dev/null || echo "nci_account not available"
echo ""

printf '\nStream directory contents (shared):\n'
if [ -d "$SHARED_CHUNKS" ] && [ "$(ls -A "$SHARED_CHUNKS" 2>/dev/null)" ]; then
    echo "Stream (shared) size: $(du -sh "$SHARED_CHUNKS" | cut -f1)"
    echo "Detailed breakdown:"
    du -sh "$SHARED_CHUNKS"/*/ 2>/dev/null | head -10 || echo "  No subdirectories"
    echo "Number of files: $(find "$SHARED_CHUNKS" -type f | wc -l)"
    echo "Manifest files:"
    find "$SHARED_CHUNKS" -name "manifest*.jsonl" -exec wc -l {} \; 2>/dev/null | head -20
    echo "Sample chunk directories:"
    find "$SHARED_CHUNKS" -type d -name "chunk_*" 2>/dev/null | head -10

    # Report file type breakdown
    echo "File type breakdown:"
    echo "  Economic loss (NPZ): $(find "$SHARED_CHUNKS" -name "*__econ.npz" | wc -l) files"
    echo "  System output (NPZ): $(find "$SHARED_CHUNKS" -name "*__sysout.npz" | wc -l) files"
    echo "  System output stats (NPZ): $(find "$SHARED_CHUNKS" -name "*__sysout_stats.npz" | wc -l) files"
    echo "  Damage states (NPZ): $(find "$SHARED_CHUNKS" -name "*__damage.npz" | wc -l) files"
else
    printf '[x] No streaming files found\n'
fi

printf '\nLog tail:\n'
if [ -f "${MODELS_DIR}/${ASSET}/output/log.txt" ]; then
    tail -n 20 "${MODELS_DIR}/${ASSET}/output/log.txt"
else
    printf '[x] Log file not found\n'
fi

# Storage usage summary
printf '\n=== Storage Usage Summary ===\n'
echo "Scratch storage usage:"
du -sh "/scratch/${PROJECT}/${USER}" 2>/dev/null || echo "  Unable to check scratch usage"
echo ""
echo "Streaming directories:"
du -sh "/scratch/${PROJECT}/${USER}"/sira_stream_* 2>/dev/null || echo "  No streaming directories found"

printf '\n=== Job finished ===\n'
